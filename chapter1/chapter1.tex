% CHAPTER_BEGIN
\section{Ajira Framework: Research Report}

\subsection{Introduction}

The \textit{ajira} project is a custom made framework for writing mainly HPC data applications. It is built on top of IPL (Ibis Portability Layer - the communication layer) and embraces a master-slave architecture. The master is basically another worker that has the extra task to setup the communication and start splitting the work between all the nodes (the workers). Thus, we do not have to install and configure anything as a separate platform and then use a special API to run jobs/tasks on top of that platform (i.e such as using Hadoop). Instead, what we need to run such an application is the \textit{ibis-server} (embedded inside the \textit{ajira} framework so we can run it from there and have our communication layer up) and, of course, a program that uses \textit{ajira} for data computations. Usually, the program does not have to be divided into a master and slave architecture. We can only use \textit{ajira} framework to write the master section. The workers' behavior is automatically dictated by the list of actions we write in the code, inside the job's description. For testing my experimental changes on the project, I have used an Ajira application that sorts large data-sets of RDF triples. To run this entire setup (\textit{ajira} + \textit{sorting application}) I have used the git repository for code deployment on DAS4 and a self written \textit{ant} file \cite{build_file} to build the application into a .jar bundle package (a package that embeds all the necessary dependencies, including the \textit{ajira.jar} package built with the default provided ant file). Next, to run the \textit{sorting application} on DAS4 a simple \textit{prun} command execution (see next section) was used.

% 
\subsection{Deployment \& execution on DAS4}

For \textit{ajira's} and \textit{sorting application's} main-code deployment on DAS4 access to git repositories \cite{arch_repo}, \cite{qpie_repo} is required. Clone the repositories in your DAS4 home directory and be sure that you are located in the benchm\_master branch. For the entire list of steps that you have to follow in order to run/execute this setup on multiple nodes, see below.
\newline
\newline
\textbf{Steps for running the \textit{sorting application} on multiple DAS4 cluster nodes:}
\begin{enumerate}
	\item Deploy \textit{ajira} and the \textit{demo sorting application} projects in your DAS4 home directory. Each project already has all the dependencies and ant files needed to built it, 
	\item Build \textit{ajira} using its ant build file: 'ant build-jar',
	\item Copy the \textit{ajira.jar} file to the \textit{lib} directory of the \textit{sorting app},
	\item Build \textit{sorting app} project using its ant build file \cite{build_file}: 'ant build-jar' (this will create a .jar file that we use to execute its main class - BenchmarkSorting - on DAS4; the .jar is placed by default in the \textit{jar} directory -- for any changes look at the parameters inside the ant file)
	\item Configure \textit{ibis-server} parameters (port, enable events, etc) from inside \textit{ajira's} ant build file,
	\item Start \textit{ibis-server} on the DAS4 head-node from the \textit{ajira} project directory: 'ant ibis-server' (note the address and port that is using),
	\item Run/execute the bundle package on DAS4 using the script example \cite{run_on_das4} -- change the parameters for your own needs.
\end{enumerate}

% 
\subsection{Workflow example: how \textit{ajira} can be used to sort RDF triples}

In this section I will explain how \textit{ajira} works on 'crunching' data, using as an example the RDF triples' \textit{distributed sorting} implemented inside the \textit{sorting example application}. To begin with, the sorting process can be split in 3 steps: (1) read and sort RDF triples; (2) send-receive communication; (3) merge and write RDF triples in files;

The main data structure used for all of the steps is the \textit{'Bucket'} -- an in-memory buffer that is filled up with triples of which hash keys are equal to the bucket's identifier. When the buffer limit size is exceeded, all of its content gets sorted and cached on the hard-disk (in temporary files). Thus, when we refer to the 'Bucket' we usually mean both the in-memory buffer and the temporary files. Each time the files' number increases above a fixed limit, a thread starts and merges them 2-by-2 up until their number gets below this limit. All the information about the files (metadata such as names, first elements, size, etc) is kept inside the Bucket -- we need those when we transfer the triples or write the results. By default, each node keeps 1 bucket in memory for every other node (remote-buckets), including itself (local-bucket). So, there are by default N buckets on each node, where N = number of nodes; a new feature allows you to use more than 1 bucket per node (NPARTITIONS\_PER\_NODE parameter, which by default is 1). The creation and initialization of a bucket (create, get, release, etc) is controlled by the 'Buckets' wrapper class.

% 
\subsubsection*{(1) Read and sort the RDF triples}

First, the data set (compressed or not) gets split into file partitions of almost the same size and each partitions is assigned to a single node for reading. The data set split is done by the ReadFromFiles action, which creates FileCollections of a minimumFileSplitSize. Next, the FileLayer of each node reads the triples from its assigned partitions -- the FileCollections, a set of path names representing the location of the files that were partitioned -- and inserts them into the corresponding local/remote bucket (where the triple's hash key equals to the node's bucket id). When a bucket gets full with triples its content is sorted using the java's default MergeSort algorithm and then cached on the disk. All of the information/metadata about the disk caching is stored inside the Bucket object for further use. If there is no more data left to read from a partition the node broadcasts a signal, alerting that the remote-buckets are ready to be transferred (this means that the send-receive start point is dependent on the size of the file partition). See Figure~\ref{fig:diag1} for a better understanding. So, at this point, the send-receive step begins -- triples are transferred from the remote-buckets to their assigned nodes (i.e. on node Y, we have a remote-bucket for node X that has triples assigned to X; these triples will be transferred to X's local-bucket).

\begin{figure}
\centering
\includegraphics[scale=0.6]{diag1}
\caption{Read and sort - remote \& local buckets structure}
\label{fig:diag1}
\end{figure}

% 
\subsubsection*{(2) Send-receive communication}

When a signal alert for a bucket ready for transfer is received by a node (\textit{receiver}), a corresponding request for fetching the triples is sent back to the \textit{sender}. Then, the \textit{sender} registers that request and, whenever the remote-bucket has available data for transfer, removes a chunk and sends it to the \textit{requester}. Next, the \textit{recipient} adds that chunk into its local-bucket and \textit{sends} a request for another fetch. As long as on the \textit{sender} side the remote-bucket still contains triples, the \textit{receiver} will get chunks transferred to its local-bucket. If the remote-bucket is empty it is released and the transfer's metatada are cleaned/removed. Nevertheless, a last signal that marks the end of the transfer is sent to the \textit{recipient}, which flags its local-bucket with \textit{finished} -- meaning that is ready for merge \& write. In Figure~\ref{fig:diag2} we show very briefly the workflow of the send-receive communication. The chunk removal, in the worst case of triples getting cached on the disk, has to extract all the minimum triples from the in-memory buffer and cached files (in sorted order) up until it fills the chunk. Is it necessary to do that because we expect to receive sorted data on the \textit{recipient} side. Otherwise,  we send a chunk directly from the in-memory buffer, after we have sorted it.

\begin{figure}
\centering
\includegraphics[scale=0.6]{diag2}
\caption{Transfer to correspondent nodes - send-receive communication}
\label{fig:diag2}
\end{figure}

% 
\subsubsection*{(3) Merge and write the RDF triples in files - results output}

When all the triples from the remote-buckets are sent (the previous step is finished), the final step merges the local-bucket's in-memory buffer with its cached content (the temporary files from the hard disk). This task is done by the same method (\textit{removeChunk()}) that removes chunks from a bucket. Now it fetches data from the local-bucket whenever the bucket-iterator is called to write data. In the end, the result of each node is represented by a single sorted RDF triples file per partition. This step is illustrated in Figure~\ref{fig:diag3}.

\begin{figure}
\centering
\includegraphics[scale=0.5]{diag3}
\caption{Merge and write}
\label{fig:diag3}
\end{figure}

\pagebreak

% 
\subsection{Execution-time}

Testing configuration: 
\begin{itemize}
	\item \textbf{Cluster-Node}: 1 Node @ DAS4, 16 x QuadCore Intel Xeon CPU E5620 @ 2.4GHz 
	\item \textbf{Number of Nodes}: 8, 12, 16
	\item \textbf{Dataset sizes}: 21, 38, 62, 149 GB
	\item \textbf{Split sizes}: 256MB, 1GB, 2GB, 4GB (0.5, 2, 4, 8 times the size of the memory buffer)
	\item \textbf{Common setups}: 512MB buffer size, available to transmit on 128MB (0.25 x buffer size), 8 concurrent tuple senders
\end{itemize}

Test - 8 nodes
\newline \newline
\renewcommand{\arraystretch}{1.2}
{\footnotesize\tt
\input{results/8n.tex}
}

Test - 12 nodes
\newline \newline
\renewcommand{\arraystretch}{1.2}
{\footnotesize\tt
\input{results/12n.tex}
}

Test - 16 nodes
\newline \newline
\renewcommand{\arraystretch}{1.2}
{\footnotesize\tt
\input{results/16n.tex}
}

% 
\subsection{Proposals \& implementations: how to improve \textit{ajira's} \\ execution-time}

In general, a distributed large scale data application is considered optimal if each data-item (in our case \textit{1 single RDF triple}) is processed using less than (inclusive) 2 writes on disk \cite{tritonsort}. In the worst case, when dealing with a big amount of data (a file partition split size bigger than the size of our available memory), the total number of disk writes goes up to 3 (1 write for reading/sorting/caching data on the disk, 1 on the recipient side for gathering data in the local-bucket and  1 for writing the final results). Given the previous research I did on Hadoop's Map-Reduce \cite{hadoop}, a platform for processing data which has a layer for data partitioning \cite{shuffling} to the reducers, I came up with the following ideas: 
\begin{itemize}
\item to \textit{disable the local sorting of remote-buckets},
\item to \textit{start as soon as possible the triples' transfer/send-receive} (we do not need to keep data more than is necessary) and
\item to \textit{use an in-background double buffering mechanism to provide final merged chunks for the bucket-iterator}. 
\end{itemize}

These proposals aim to increase the chances of using less then 3 writes on disk (unless the network is a bottleneck) and to improve the execution time of the \textit{ajira} framework. From a distributed-sorting application point of view, this approach of moving the sort phase to the recipient side with the consequence of having less writes to disks may improve the total time of the execution. In the following paragraphs I will discuss each proposal and also provide some implementation aspects.

% 
\subsubsection*{(1) Disabling sort property for remote-buckets}

Because we start the up-call (data-transfer) at the moment we begin the read phase, we do not need to sort before-hand the remote-buckets. Instead, we apply the sorting on the recipient side (local-bucket), under the same assumptions -- when the buffer fills up, we sort and cache its content on the disk. To disable sorting on the remote-buckets, we have to construct/initialize them without the sorting function and sorting parameters.

% 
\subsubsection*{(2) Interleave the transfer with the local reading}

Due to the early start of the transfer, it is obvious that now the send-receive overlaps with the files-partition read phase. So, during the startTransfer() call we also have to broadcast signal alerts to the other nodes to have them send back requests for fetching data (the alertTransfer() method). To enable that we have to accomplish the followings:

\textbf{(a) Allow receiving of unsorted data}
Previously, when a chunk was transferred (\textit{sendTuples() method}) we specified the sorting function and its parameters in the message. Because we do not want to sort the remote-buckets anymore, we set a null sorting function and parameters. Thus, on the recipient side we have to allow the append of unsorted data into the local-bucket. The original version of \textit{addAll()} method, which adds chunks into the local-bucket, expected sorted data -- it compares the size of the chunk with the size of the buffer and depending on which one is bigger that one is cached on the disk (\textit{cacheBuffer() method}). If the chunk is bigger we cache directly without sorting (we already have done that on the sender side), otherwise we sort the buffer and spill it on the disk. This logic is now changed a bit -- we first try to add the chunk into the buffer, or the other way around if the buffer has fewer elements, instead of writing directly to the disk. Only if that fails, we sort and cache in a temporary file the bigger one. 

\textbf{(b) Split the main buffer into two buffers}

Furthermore, we can improve the previous idea by splitting the main buffer into two separate buffers in such way that one should be use for reading (\textit{inBuffer}) and one for receiving chunks (\textit{exBuffer}). With this approach, we can operate independently on each buffer. One disadvantage is that we require extra memory for the second buffer. At the end of the transfer -- when the bucket is flagged with finished -- we merge one buffer into the other (\textit{combineInExBuffers()} method) in order to have a single buffer combined with both information. If both are big, one of them (the smaller one) is cached on the disk.

\textbf{(c) Loosen-up the synchronization between bucket's methods}

Moreover, in addition to splitting the main buffer into two independent buffers, we can loosen-up the synchronization primitives between the read and transfer (\textit{add() and addAll() methods}), the only coarse synchronization point being the moment that both methods call \textit{cacheBuffer()} (both buffers might get full at the same time).

Steps 1 and 2 are described in Figure~\ref{fig:diag4}.

\pagebreak

\begin{figure}
\centering
\includegraphics[scale=0.5]{diag4a}
\linebreak
\linebreak
\centering
\includegraphics[scale=0.5]{diag4b}
\caption{New local/remote buckets}
\label{fig:diag4}
\end{figure}

% 
\subsubsection*{(3) Double-buffering for writing the results in files}

The last proposal concerns the writing of the sorted local-bucket (all the triples) into the final result file. This phase uses a \textit{bucketIterator} to iterate over the results. As in turn the iterator uses multiple \textit{removeChunk()} method calls to fetch chunks from the local-bucket when there are no more triples to iterate/write down. An idea would be to "hide" the time that the iterator has to wait for merging-up triples from the main memory (buffer) with the ones stored on the disk. Instead we can try to remove/get a chunk that has been already processed and stored in a set of auxiliary buffers in memory (triples ready to 'serve' the iterator). To implement this, we start a separate thread when we set the bucket's flag on finished, to fill up auxiliary buffers with chunks from the main buffer. When a write is performed the iterator is called and the chunk is taken directly from the auxiliary buffers instead of waiting for the whole merging phase. Moreover, when a buffer from the set empties the thread will go on and try to refill it, if there are triples available -- this mechanism of storing buffers that were pre-processed is called 'Double-Buffering'. Figure~\ref{fig:diag5} refers to \textit{removeChunk()} call mechanism explained in this paragraph. 

\begin{figure}
\centering
\includegraphics[scale=0.6]{diag5}
\caption{In-background merge-up}
\label{fig:diag5}
\end{figure}

% 
\subsubsection{Results: execution-time}

Test - 8 nodes
\newline \newline
\renewcommand{\arraystretch}{1.2}
{\footnotesize\tt
\input{results/8n_r.tex}
}

\newpage

Test - 12 nodes
\newline \newline
\renewcommand{\arraystretch}{1.2}
{\footnotesize\tt
\input{results/12n_r.tex}
}

Test - 16 nodes
\newline \newline
\renewcommand{\arraystretch}{1.2}
{\footnotesize\tt
\input{results/16n_r.tex}
}

\newpage

Speedup test: average on the best 10 execution-times (mm.ss), see Figure~\ref{fig:speedup}
\newline \newline
\renewcommand{\arraystretch}{1.2}
{\footnotesize\tt
\input{results/speedup.tex}
}

\begin{figure}
\centering
\includegraphics[scale=0.6]{speedup}
\caption{Speedup-chart for Ajira[blue] \& Ajira-Research[red]}
\label{fig:speedup}
\end{figure}

% 
\subsubsection*{Discussion}

What can we reason from the time results and improvement percentages is that there is an actually trade-of in performance between starting the transfer as soon as possible and starting after we read the first split of files (many small transfers, less to no W to disk VS. less bigger transfers, moderate to many W to disk). Notably, it depends a lot on the right parameter tuning over the cluster setup (network vs. storage, which one is faster?). Comparing the first two tables (8 and 12 nodes with 8 concurrent tuple senders) and also taking into account the speedup chart, we notice that our improvement increases for  tests with a 256MB and 1GB file split sizes and data set up to a 62GB size. Unfortunately, we cannot say the same about 62GB or a bigger data set and split size where we obtained a lower or even no improvements over the original version of Ajira. One possible cause would be that for 8 - 12 nodes a 4GB split size does not add an big overhead when we sort a data set of 149GB. We do have to wait for reading a 4GB file split (file partition) before starting the transfer, but for a 35x times bigger dataset and only for 8 - 12 nodes that is not overwhelming. In contrast, on the last table we achieved exactly the opposite. On the first 5 columns, 8 concurrent tuple senders for 16 nodes seems not to be enough to overcome the overhead caused by the file split. In this case the overhead is almost nonexistent because the tuples partitioning happens between more nodes than the previous tests. On the other hand, we see an increase of improvement over the last 3 columns, meaning that it makes sense not to wait for all 16 nodes to finish their 2GB - 4GB file partition read. 

\subsection{Future work}

For future work I propose the following developments:

\begin{itemize}
	\item \textit{Moving to processes.} One idea is to implement an option/feature that allows specifying how many 'instances' of PartitionToNodes to run locally and how many remotely on other nodes (following the Apache design model, i.e multiple mappers on same node). Example: if 1 node has 4 x QuadCore CPUs --- in total we can run 8 instances at least (maximum 16), 2 local instances on every node (given the fact that one instance makes highly use of multi-threading is fair enough to start only 2 processes per node).
	\item \textit{Thread synchronization.} In the Ajira framework we use threads for background processing and all the methods tend to be globally synchronized to avoid race conditions on the data structures. This adds a significant overhead to the application, because the access to the data structures becomes coarse grained. A solution for that is to use fine-grained/lock-free data structures or a mechanism such as double buffering. 
	\item \textit{Fine-grained file splitting.} In Ajira we coarsely split the data set between nodes. That means we consider an entire file when we decide where to send it (in what split collection), for further computations, instead of splitting the file in multiple pieces (i.e 64MB) and take their sizes into consideration. For small size files (with an order of maximum tens of MB) the coarse grained level works fine, but if the dataset has bigger files in small numbers we can get into an unbalanced situation.
	\item \textit{Data sampling.} Currently, the output of a distributed sorting application is not globally ordered. It means that we still have to merge each result file in order to get a full sorted data set from start to its end. Instead, a well known method to achieve global ordering and also fair data balancing is to perform a previous step called \textit{sampling}. A state-of-the-art idea is to rely on a Trie (a prefix tree) based algorithm with a depth level fixed in a way that will assure you enough granularity to equally balance the key partitions to all your nodes. When the tree is computed it can be used as a tuple partitioner instead of a hash algorithm in your normal data computation. An example of a Trie based sampling algorithm applied with Hadoop Map-Reduce is described in \cite{terasort}.
\end{itemize}

\subsection{Conclusions}

\begin{itemize}
	\item For batch processing of huge amount of data is still better to use the hard-disk to improve the execution time -- IO bound. So, what it matters now is how fast and when we should perform R/W operations comparing with other frameworks. On the other hand, for real-time processing the research approach is a better match -- network bound (not the case for distributed sorting which is an example of a batch processing).
	\item Probably an adaptive mechanism -- changing the file split size limit depending on the data set size automatically or exposing that to the user's decision -- would cover both explained scenarios
	\item Finally, both approaches got decent time executions and scaling-out performances for small, moderate and bigger data sets. Nevertheless, there is still space for further optimization. 
\end{itemize}

% CHAPTER_END