% CHAPTER_BEGIN
\section{Arch Framework: Research Report}

\subsection{Introduction}

The \textit{arch} project represents a custom made framework for writing mainly HPC data applications. Is built as a framework on top of IPL (Ibis Portability Layer - communication layer) adopting a master-slave architecture. The master is basically another slave that hast the extra task to setup the communication and start splitting the job between all the nodes (slaves). Thus, we do not have to install and configure anything as an independent platform and then use a special API to run jobs/tasks on top of that platform (i.e such as using Hadoop). With other words, what we need to run an \textit{arch} application is the \textit{ibis-server} (it is embedded inside the \textit{arch} framework so we can run it from there and have our inter nodes communication layer) and, of course, a program that uses \textit{arch} to compute some data. Usually, the program does not have to be divided into a master and slave side. We can use \textit{arch} framework only to write the master section. The slaves' behavior is automatically dictated by the list of actions that we add in the master code, inside the job. For testing my experimental changes on \textit{arch} I have run \textit{querypie:BenchmarkSorting}, an application built with \textit{arch} framework that sorts large data-sets of RDF triples. To build and run this entire setup (\textit{arch} + \textit{querypie}) I have used the git repository for code deployment on DAS4 and a self written ant file \cite{build_file} to build \textit{querypie} into a .jar bundle package (a package that embeds inside all the necessary dependencies, including the \textit{arch.jar} package built with the default ant file). Next, to run the \textit{querypie} on DAS4 was a simple \textit{prun} command execution (see next section).

% 
\subsection{Deployment \& execution on DAS4}

For \textit{arch} and \textit{querypie} main-code deployment on DAS4 you will first need access to their git repositories \cite{arch_repo}, \cite{qpie_repo}. After that, simply clone their repositories in your home DAS4 directory and be sure that you are located in the 'master' branch. For the entire list of steps that you have to follow in order to run/execute this setup on multiple nodes, read next.
\newline
\newline
\textbf{Steps for running \textit{querypie} on multiple DAS4 cluster nodes:}
\begin{enumerate}
	\item Deploy \textit{arch} and \textit{querypie} projects in your home DAS4 directory. Each project already has all the dependencies packages and ant files necessary for automatic build. 
	\item Build \textit{arch} project using its ant build file: 'ant build-jar' 
	\item Copy the \textit{arch.jar} package in the \textit{/lib} directory of the \textit{querypie}
	\item Build \textit{querypie} project using its ant build file \cite{build_file}: 'ant build-jar' (will create a .jar bundle package that we use to execute its main class - BenchmarkSorting - on DAS4; the .jar is placed by default in the \textit{/jar} directory -- for any change you wish to do look for the parameters inside the ant file)
	\item Configure \textit{ibis-server} parameters (port, enable events, etc) from inside \textit{arch's} ant build file 
	\item Start \textit{ibis-server} on the DAS4 head-node from the \textit{arch} project directory: 'ant ibis-server' (note the address and port that is using)
	\item Run/execute \textit{querypie} on DAS4 using the script example \cite{run_on_das4} -- change the parameters for your own needs.
\end{enumerate}

% 
\subsection{Workflow example: how \textit{arch} can be used to sort RDF triples}

In this section I will explain how the \textit{arch} framework works on 'crunching' data, using as a workflow example the RDF triples' distributed sorting implemented in \textit{querypie}, BenchmarkSorting. To begin with, the process of sorting can be split in 3 steps: (1) read and sort the RDF triples; (2) send-receive communication; (3) merge and write the RDF triples in files;

The main data structure used in all three steps is the 'Bucket' -- an in-memory buffer that is filled up with triples of which hash keys are equal to the bucket's identifier. When the buffer limit size is exceeded, all its content gets sorted and cached on the hard-disk (in files). Thus, when we refer to the 'Bucket' we usually mean both the in-memory buffer and the hard-disk stored files (if data hits the disk). Each time the files' number increases above a fixed limit, a thread is started to merge them 2-by-2 up until their number gets below. All the information about the files (metadata such as names, first elements, size, etc) are kept inside the Bucket -- we need them at the time we transfer the triples or when we write the sorted set. By default, each node keeps 1 bucket in the memory for every other node (remote-buckets), including itself (local-bucket). So, there are by default N buckets on each node, where N = number of nodes; a new feature allows you to use more than 1 bucket per node (NPARTITIONS\_PER\_NODE parameter, which by default is 1). The creation and initialization of a bucket (create, get, release, etc) is controlled by the 'Buckets' wrapper class.

% 
\subsubsection*{(1) Read and sort the RDF triples}

First, the data set (compressed or not) gets split in file partitions of almost same sizes, each one assigned to one node for further reading. The split of the data set is done by the ReadFromFiles action, which creates FileCollections of a 'minimumFileSplitSize'. Next, the FileLayer of each node reads the triples from its assigned partitions - FileCollection (a set of pathnames representing the partitioned files) and inserts them into the corresponding node's remote-bucket (where hash key of the triple equals to the node's remote-bucket id). If a bucket gets full with triples, its content is sorted using the java MergeSort algorithm and cached on disk. All the information/metadata about caching that content on the disk is stored inside the Bucket object for further use. When there is no more data left to read from a partition, the node broadcasts a signal, alerting that the remote-buckets are ready to get transferred (this means that the start of the send-receive step is dependent on the size of the file partition). So, at this point, the send-receive step starts -- triples are transferred from the remote-buckets to their assigned nodes (i.e. on node Y, the remote-bucket for node X has triples assigned to X => triple from Y's remote-bucket will be transferred to X's local-bucket).

<Diagram-1>

% 
\subsubsection*{(2) Send-receive communication}

When a signal alert for a bucket ready for transfer is received by a node, the \textit{receiver}, a corespondent request for fetching triples from it is sent back to the \textit{sender}. Then, the \textit{sender} registers that request and, whenever the remote-bucket has the available data (MIN\_SIZE\_TO\_SEND) for transfer, removes a chunk and sends it to the \textit{requester}. Next, the \textit{recipient} adds that chunk into its local-bucket and \textit{sends} a request for another fetch. As long as on the other side (\textit{sender's side}) the remote-bucket still contains triples, the \textit{receiver} will have chunks get transferred in its local-bucket. If there are no more RDF triples inside of it, the remote-bucket is released and the transfer's metatada are cleaned/removed. But, before that, a last signal that marks the end of the transfer is sent to the \textit{recipient}, which flags its local-bucket with 'finished' -- meaning that is ready for merge \& write. The chunk removal, in worst case when triples are getting cached on the disk, has to extract all the minimums from the in-memory buffer and the cached files up until it fills the chunk's buffer. Is necessary to do that because we expect to receive sorted data on the recipient side.

<Diagram-2>

% 
\subsubsection*{(3) Merge and write the RDF triples in files}

After all the triples got moved from the remote-buckets (previous step is finished), the final step is all about merging the local-bucket's in-memory buffer with its cached content (the files from the hard disk) -- in case data hits the disk. This task is done by the same method (RemoveChunk) that removes chunks for triples' transfer. Now it stands for fetching data from the local-bucket whenever the bucket-iterator is called for writing data. In the end, the result at each node is represented by one sorted file with RDF triples (only if we let the default number, equal with 1, of buckets per node).

<Diagram-3>

% 
\subsection{Execution-time \& profiling}

%BucketSize: 512MB
%MinimumDataToSend: 128MB
%#Threads: 8, 12, 16
%DataSize1: 21, 38, 62 GB
%DataSize2: 120+ GB
%Splits1: 256MB, 1GB, 2GB
%Splits2: 4GB

<Table-1>
<Chart-1>

% 
\subsubsection*{Discussion}

% 
\subsection{Proposals \& implementations: how to improve \textit{arch's} execution-time}

% 
\subsubsection*{(1) Disabling sort property for remote-buckets}

<Diagram-4>

% 
\subsubsection*{(2) Interleave send-receive with local reading}

\textbf{(a) Allow receiving of unsorted data}

\textbf{(b) Split 'tuples' buffer into two buffers}

\textbf{(c) Loosen-up the synchronization between bucket's methods}

<Diagram-5>

% 
\subsubsection*{(3) In-background chunks' removal for writing the results in files}

<Diagram-6>

% 
\subsubsection{Results: execution-time \& profiling}

<Table-2>
<Chart-2>

% 
\subsubsection*{Discussion}

\subsection{Future work}

\subsection{Conclusions}
% CHAPTER_END
